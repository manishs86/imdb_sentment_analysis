{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1154310f0>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from  tqdm import tqdm\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "import re\n",
    "from torchtext import vocab as vc\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdb_dir = '/Users/sampath/dlProjects/imdb_sent/aclImdb'\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "test_dir = os.path.join(imdb_dir, 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Load the each text into texts list\n",
    "## corresponding label will be in labels list. \n",
    "def read_test_train_dir(path,):\n",
    "    labels = []\n",
    "    texts = []\n",
    "    for label_type in ['neg', 'pos']:\n",
    "        dir_name = os.path.join(train_dir, label_type)\n",
    "        for fname in os.listdir(dir_name):\n",
    "            if fname[-4:] == '.txt':\n",
    "                f = open(os.path.join(dir_name, fname))\n",
    "                texts.append(f.read())\n",
    "                f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "    return texts,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_texts,train_labels = read_test_train_dir(train_dir)\n",
    "test_texts, test_labels = read_test_train_dir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_paragraph_words(text):\n",
    "    return (flatten([word_tokenize(s) for s in sent_tokenize(text)]))\n",
    "\n",
    "sent_tokenize = nltk.sent_tokenize\n",
    "word_tokenize = RegexpTokenizer(r'\\w+').tokenize\n",
    "\n",
    "def word_tokenize_para(text):\n",
    "    return [word_tokenize(s) for s in sent_tokenize(text)]\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create vocabulary\n",
    "vocab_counter = Counter(flatten([get_paragraph_words(text) for text in train_texts]))   \n",
    "\n",
    "#get w2v populated for vocabulary\n",
    "w2v = vc.Vocab(vocab_counter,max_size=50000,min_freq=5,vectors='glove.6B.50d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# randomly shuffle the training data\n",
    "train_comb = list(zip(train_texts,train_labels))\n",
    "random.shuffle(train_comb)\n",
    "train_texts2,train_labels2 = zip(*train_comb)\n",
    "maxSeqLength = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(w2v, text,maxSeqLength):\n",
    "    #trim the sentence to maxSeqLength, otherwise return with original length. \n",
    "    return [w2v.stoi[word] for word in get_paragraph_words(text)[0:maxSeqLength]]\n",
    "\n",
    "# pads a tensor of shape [num_words,1,embeddings_dim] to a maximum sentence length given by length argument. \n",
    "# it takes a tensor, keeps adding zero rows till length\n",
    "def pad(tensor, length):\n",
    "        return torch.cat([tensor, tensor.new(length - tensor.size(0), *tensor.size()[1:]).zero_()])\n",
    "\n",
    "def variableFromSentence(w2v,text,maxSeqLength):\n",
    "    indexes = indexesFromSentence(w2v, text,maxSeqLength)\n",
    "    #returns tensor with size [num_words,1,embedding_dim]\n",
    "    #That extra 1 dimension is because PyTorch assumes everything is in batches - weâ€™re just using a batch size of 1 here.\n",
    "    sent_word_vectors = torch.cat([w2v.vectors[i].view(1,-1) for i in indexes]).view(len(indexes),1,-1)\n",
    "    \n",
    "    #pad 500 , output is 500,1,embedding_dim\n",
    "    #sent_word_vectors = pad(sent_word_vectors,maxSeqLength)\n",
    "    \n",
    "    #batch first (1,500,embedding_dim)\n",
    "    sent_word_vectors = sent_word_vectors.view(1,len(sent_word_vectors),-1)\n",
    "    \n",
    "    return sent_word_vectors \n",
    "\n",
    "def variablesFromPair(pair):\n",
    "    input_variable = variableFromSentence(w2v, pair[0])\n",
    "    target_variable = pair[1]\n",
    "    return (input_variable, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = [variableFromSentence(w2v,text,maxSeqLength) for text in train_texts2]\n",
    "train_targets = train_labels2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 195, 50])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectors[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_paragraph_words(train_texts2[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Classifier(nn.Module):\n",
    "    def __init__(self, input_dim,context_dim,num_classes):\n",
    "        super(RNN_Classifier,self).__init__()        \n",
    "        self.context_dim = context_dim;\n",
    "        self.gru = nn.GRU(input_dim,context_dim,1,bias=True,batch_first=True)\n",
    "        self.linear = nn.Linear(context_dim,num_classes);\n",
    "   \n",
    "    def forward(self,input):\n",
    "        #we dont need to initialize explicitly - \n",
    "        #h0 = Variable(torch.zeros(1,input.size(0),self.context_dim))\n",
    "        all_h, last_h = self.gru(input);\n",
    "        #since we have only 1 layer and 1 direction\n",
    "        output = self.linear(last_h[0]);\n",
    "        return output;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = 50\n",
    "context_dim = 64\n",
    "num_classes = 2\n",
    "model =  RNN_Classifier(input_dim,context_dim,num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  \n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "batch_size = 50\n",
    "num_epochs = 25000//batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(str_idx,end_idx):\n",
    "        input_vectors = train_vectors[str_idx:end_idx]\n",
    "        labels = Variable(torch.LongTensor(train_labels2[str_idx:end_idx]))\n",
    "        seq_lens = torch.LongTensor([i.shape[1] for i in input_vectors])\n",
    "        embedding_dim = train_vectors[0].shape[2]\n",
    "        #batch_inputs  - [batch_size, seq_len,embedding_dim]\n",
    "        batch_inputs = Variable(torch.zeros((len(seq_lens), seq_lens.max(),embedding_dim)))\n",
    "        for idx,(seq,seqlen) in enumerate(zip(input_vectors,seq_lens)):\n",
    "            batch_inputs[idx,:seqlen] = seq\n",
    "        seq_lens, perm_idx = seq_lens.sort(0, descending=True)\n",
    "        batch_inputs = batch_inputs[perm_idx]\n",
    "        batch_inputs = pack_padded_sequence(batch_inputs, seq_lens.numpy(),batch_first=True)\n",
    "        labels = labels[perm_idx]\n",
    "        return(batch_inputs,labels)\n",
    "        #assign each training vector to left \n",
    "        \n",
    "        #print(seq_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/500],  Loss: 0.7227\n",
      "Epoch [200/500],  Loss: 0.7264\n",
      "Epoch [300/500],  Loss: 0.7301\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    str_idx = epoch * batch_size\n",
    "    end_idx = (epoch+1) * batch_size\n",
    "    \n",
    "    inputs,labels = get_batch(str_idx,end_idx)\n",
    "    \n",
    "    # Forward + Backward + Optimize\n",
    "    optimizer.zero_grad()  # zero the gradient buffer\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print ('Epoch [%d/%d],  Loss: %.4f' \n",
    "               %(epoch+1, num_epochs, loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the  train reviews: 55 %\n"
     ]
    }
   ],
   "source": [
    "## Test the Model on training data\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "testing_inputs = Variable(torch.cat(train_vectors))\n",
    "testing_labels = torch.LongTensor(train_targets)\n",
    "outputs = model(testing_inputs)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "total = testing_labels.size(0)\n",
    "correct = (predicted == testing_labels).sum()\n",
    "\n",
    "\n",
    "print('Accuracy of the network on the  train reviews: %d %%' % (100 * correct / total))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 25000 test images: 50 %\n"
     ]
    }
   ],
   "source": [
    "## Test the Model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "test_vectors = [variableFromSentence(w2v,text,maxSeqLength) for text in test_texts]\n",
    "testing_inputs = Variable(torch.cat(test_vectors))\n",
    "testing_labels = torch.LongTensor(test_labels)\n",
    "outputs = model(testing_inputs)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "total = testing_labels.size(0)\n",
    "correct = (predicted == testing_labels).sum()\n",
    "\n",
    "\n",
    "print('Accuracy of the network on the 25000 test images: %d %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2);\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss and Optimizer\n",
    "net = Net(25000,500,2)\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  \n",
    "batch_size = 50\n",
    "num_epochs = 25000//50\n",
    "len(train_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c5fc81ad60ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstr_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "net.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    str_idx = epoch * batch_size\n",
    "    end_idx = (epoch+1) * batch_size\n",
    "    input_tensor = torch.cat(train_vectors[str_idx:end_idx]);\n",
    "    inputs = Variable(input_tensor.view(input_tensor.shape[0],-1))\n",
    "    labels = Variable(torch.LongTensor(train_labels2[str_idx:end_idx]));\n",
    "    \n",
    "    # Forward + Backward + Optimize\n",
    "    optimizer.zero_grad()  # zero the gradient buffer\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "               %(epoch+1, num_epochs, i+1, len(train_vectors)//batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 25000 test images: 79 %\n"
     ]
    }
   ],
   "source": [
    "## Test the Model\n",
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "test_vectors = [variableFromSentence(w2v,text,maxSeqLength) for text in test_texts]\n",
    "test_input_tensor = torch.cat(test_vectors)\n",
    "test_label_subset = torch.LongTensor(test_labels)\n",
    "\n",
    "test_inputs = Variable(test_input_tensor.view(test_input_tensor.shape[0],-1))\n",
    "outputs = net(test_inputs)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "total = test_label_subset.size(0)\n",
    "correct = (predicted == test_label_subset).sum()\n",
    "\n",
    "\n",
    "print('Accuracy of the network on the 25000 test images: %d %%' % (100 * correct / total))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
